import torch
from pdf2image import convert_from_path
from colpali_engine.models import ColQwen2_5, ColQwen2_5_Processor
from transformers.utils.import_utils import is_flash_attn_2_available
from transformers import Qwen2_5_VLForConditionalGeneration, AutoTokenizer, AutoProcessor
from qwen_vl_utils import process_vision_info
import os
from tqdm.auto import tqdm
import gc
from openai import OpenAI
from IPython.display import display
import re
import html
import streamlit as st
os.environ['HF_HOME'] = "/teamspace/studios/this_studio/huggingface_cache"


def render_markdown_with_math(text):
    # Escape HTML characters (important!)
    text = html.escape(text)

    # Replace [math] with \(math\) for inline LaTeX rendering
    text = re.sub(r"\[\s*(.*?)\s*\]", r"\\(\1\\)", text)

    # Replace $$...$$ and \[...\] and \(..\) etc. (leave them unchanged — MathJax handles it)
    return f"<div>{text}</div>"
    
@st.cache_data
def load_images(documnet_path):
    images = convert_from_path(documnet_path)
    return images

@st.cache_resource
def load_retrieval_model(retrieval_model_name,torch_dtype,device_map,attn_implementation):
    retrieval_model = ColQwen2_5.from_pretrained(retrieval_model_name,torch_dtype = torch_dtype,device_map = device_map,attn_implementation = attn_implementation).eval()
    retrieval_processor = ColQwen2_5_Processor.from_pretrained(retrieval_model_name)
    return retrieval_model,retrieval_processor

@st.cache_resource
def load_vlm_model(vlm_model_name,torch_dtype,device_map):
    vlm_model = Qwen2_5_VLForConditionalGeneration.from_pretrained(vlm_model_name,torch_dtype = torch_dtype,device_map = device_map)
    vlm_processor = AutoProcessor.from_pretrained(vlm_model_name)
    return vlm_model,vlm_processor

def get_query():
    query = [input("your_query: ")]
    return query

def query_enhancer(query:str,api_key:str):
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )
    completion = client.chat.completions.create(
      # model="mistralai/mistral-small-3.2-24b-instruct:free",
      model = "deepseek/deepseek-r1-0528:free",
      messages=[
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": f"""
                        You are an advanced AI assistant specializing in information extraction. 
                        Your task is to analyze the following query and extract the most relevant 
                        and concise keywords or key phrases that would optimize retrieval in a RAG (Retrieval-Augmented Generation) system. 
                        Only return the extracted keywords or key phrases, separated by commas, and nothing else. Query: {query}
                      """
            },
          ]
        }
      ]
    )
    return completion.choices[0].message.content

def answer_checker(query:str,answer:str,api_key:str):
    client = OpenAI(
        base_url="https://openrouter.ai/api/v1",
        api_key=api_key,
    )
    completion = client.chat.completions.create(
      # model="mistralai/mistral-small-3.2-24b-instruct:free",
      model = "deepseek/deepseek-r1-0528:free",
      messages=[
        {
          "role": "user",
          "content": [
            {
              "type": "text",
              "text": f"""
                        You are an advanced AI assistant specializing in answer evaluation. 
                        Analyze the following user query and the corresponding answer generated by a vision-language model. 
                        Determine whether the answer fully and accurately addresses the user’s query. 
                        Respond with "Yes" if the answer is relevant and complete, or "No" if it is not. 
                        Only return "Yes" or "No" as your response.
                        User Query: {query}
                        Model Answer: {answer}
                      """
            },
          ]
        }
      ]
    )
    return completion.choices[0].message.content
    
def retrieval(k:int, queries:list,images,retrieval_model,retrieval_processor,device_map):
    scores_list = []
    for image in tqdm(images):
        batch_image = retrieval_processor.process_images([image]).to(retrieval_model.device)
        for query in queries:
            batch_query = retrieval_processor.process_queries([query]).to(retrieval_model.device)
            with torch.no_grad():
                image_embedding = retrieval_model(**batch_image)
                query_embedding = retrieval_model(**batch_query)
            score = retrieval_processor.score_multi_vector(query_embedding,image_embedding)
            scores_list.append(score.item())
    gc.collect()
    torch.cuda.empty_cache()
    top_result = torch.topk(torch.tensor(scores_list),k = k)
    topk_scores = top_result.values
    topk_index = top_result.indices
    return topk_scores,topk_index

def augmentation_and_generation(queries:list,images,topk_scores,topk_index,vlm_model,vlm_processor,device_map):
    gc.collect()
    torch.cuda.empty_cache()
    messages = [{
        "role": "user",
        "content": [
            {"type": "image", "image": images[topk_index[0]]},
            {"type": "image", "image": images[topk_index[1]]},
            {"type": "image", "image": images[topk_index[2]]},
            {"type": "text", "text": queries[0]}
                   ],
    }]
    text = vlm_processor.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    image_inputs, video_inputs = process_vision_info(messages)
    inputs = vlm_processor(
        text=[text],
        images=image_inputs,
        videos=video_inputs,
        padding=True,
        return_tensors="pt",
    )
    inputs = inputs.to(device_map)
    
    # Inference: Generation of the output
    generated_ids = vlm_model.generate(**inputs, max_new_tokens=10000)
    generated_ids_trimmed = [
        out_ids[len(in_ids) :] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
    ]
    output_text = vlm_processor.batch_decode(
        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False
    )
    gc.collect()
    torch.cuda.empty_cache()
    return output_text